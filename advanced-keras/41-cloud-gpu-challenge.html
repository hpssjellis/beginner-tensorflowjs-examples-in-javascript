<html>
  <head>
    <title>MNIST in TensorFlow.js Layers API</title>
    <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.cyan-teal.min.css" />
   
  
  
  </head>
  
  
  <body>
    <style>
      #content {
        padding: 15px;
      }
      body, input, select, button {
        font-family: monospace;
      }
      canvas {
        width: 200px;
      }
      .prediction-canvas{
        width: 100px;
      }
      .pred {
        font-size: 20px;
        line-height: 25px;
        width: 100px;
      }
      .pred-correct {
        background-color: #00cf00;
      }
      .pred-incorrect {
        background-color: red;
      }
      .pred-container {
        display: inline-block;
        width: 100px;
        margin: 10px;
      }
      #lossCanvas > div, #accuracyCanvas > div {
        display: none;
      }
      .canvases {
        display: inline-block;
      }
      .hparam-label {
        display: inline-block;
        width: 250px;
      }
    </style>

    <div id="content">
      <h3>Cloud GPU / TPU TFJS Challenge. </h3>
      Train and Save this Tensorflowjs Model on your Cloud service and tweet to @rocksetta the lowest training time (in seconds) and 
      the lowest cost (in pennies)<br><br>
      When you are done, tweet something like:<br>
      Note: I have made this model a fair bit bigger than needed to drag out the training session.<br>
      
<textarea rows="5" cols="60">Hey @rocksetta here is my #cloudTfjsChallenge results:    
Cloud service website (as a URL),   
train time (in seconds),  
cost (in pennies), 
Optional: saved model (as a URL)
</textarea><br><hr><br>

      
    
      <div>
        <div style="display:none">
          <span class="hparam-label">Model Type:</span>
          <select id="model-type">
            <option value="ConvNet">ConvNet with Dropout</option>
          </select>
        </div>

        <div style="display:none">
          <span class="hparam-label" ># of training epochs:</span>
          <input id="train-epochs" value="1">
        </div>

        <button id="train">Load MNIST Data, Train Model and auto Save</button>
      </div>

      <div id="status"></div>
      <div id="message"></div>

      <div id="stats">
        <div class="canvases">
          <div class="label" id="loss-label"></div>
          <div id="loss-canvas"></div>
        </div>
        <div class="canvases">
          <div class="label" id="accuracy-label"></div>
          <div id="accuracy-canvas"></div>
          <div id="myDurationOut"></div>
          
          
          
        </div>
      </div>

      <div id="images"></div>
    </div>
    <hr>

  Test loading your model<br>
  Enter your online saved model here: <input id="myInFile" size=120 type=text value="https://hpssjellis.github.io/beginner-tensorflowjs-examples-in-javascript/saved-models/mnist/convnet/my-mnist01.json"><br>
<input type=button value="Load Model" onclick="{myLoadUrl()}"><br><br>  
    
    <div id="myDivTest">...</div>
    <div id="myDivSummary">...</div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.0"> </script> 


   
     <script>
  
 myLoadUrl = async function(){
     //const startTime = performance.now();
  //alert('The test function will need to be changed if other models are loaded')                                                                                             
  document.getElementById('myDivTest').innerHTML = 'Expect major code changes if you load a different model than what is expected'  
  const myFileName = document.getElementById('myInFile').value
  if (myFileName != null){  
    const model = await tf.loadModel(myFileName);     // should make the model a global variable
    document.getElementById('myDivSummary').innerHTML = ''      
    await model.summary(null,null,x => {document.getElementById('myDivSummary').innerHTML += x + '<br>'});
    //const endTime = performance.now();
   // document.getElementById('myDivSummary').innerHTML += '<hr>Train duration was:'+ ((endTime-startTime)*1000).toFixed(0) + ' seconds<br>'
   // await myPredict()
    await load();
    showPredictions(model)
  }                                                                           
}                                                                                             
        
 

const IMAGE_H = 28;
const IMAGE_W = 28;
const IMAGE_SIZE = IMAGE_H * IMAGE_W;

//const IMAGE_SIZE = 784;
const NUM_CLASSES = 10;
const NUM_DATASET_ELEMENTS = 65000;

const NUM_TRAIN_ELEMENTS = 55000;
const NUM_TEST_ELEMENTS = NUM_DATASET_ELEMENTS - NUM_TRAIN_ELEMENTS;

const MNIST_IMAGES_SPRITE_PATH =
    'https://storage.googleapis.com/learnjs-data/model-builder/mnist_images.png';
const MNIST_LABELS_PATH =
    'https://storage.googleapis.com/learnjs-data/model-builder/mnist_labels_uint8';






/**
 * A class that fetches the sprited MNIST dataset and returns shuffled batches.
 *
 * NOTE: This will get much easier. For now, we do data fetching and
 * manipulation manually.
 */






class MnistData {
  constructor() {
   // this.shuffledTrainIndex = 0;
   // this.shuffledTestIndex = 0;
  }

  async load() {
    // Make a request for the MNIST sprited image.
    const img = new Image();
    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d');
    const imgRequest = new Promise((resolve, reject) => {
      img.crossOrigin = '';
      img.onload = () => {
        img.width = img.naturalWidth;
        img.height = img.naturalHeight;

        const datasetBytesBuffer =
            new ArrayBuffer(NUM_DATASET_ELEMENTS * IMAGE_SIZE * 4);

        const chunkSize = 5000;
        canvas.width = img.width;
        canvas.height = chunkSize;

        for (let i = 0; i < NUM_DATASET_ELEMENTS / chunkSize; i++) {
          const datasetBytesView = new Float32Array(
              datasetBytesBuffer, i * IMAGE_SIZE * chunkSize * 4,
              IMAGE_SIZE * chunkSize);
          ctx.drawImage(
              img, 0, i * chunkSize, img.width, chunkSize, 0, 0, img.width,
              chunkSize);

          const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

          for (let j = 0; j < imageData.data.length / 4; j++) {
            // All channels hold an equal value since the image is grayscale, so
            // just read the red channel.
            datasetBytesView[j] = imageData.data[j * 4] / 255;
          }
        }
        this.datasetImages = new Float32Array(datasetBytesBuffer);

        resolve();
      };
      img.src = MNIST_IMAGES_SPRITE_PATH;
    });

    //const labelsRequest = fetch(MNIST_LABELS_PATH, {mode: 'arraybuffer'});
    const labelsRequest = fetch(MNIST_LABELS_PATH);
    const [imgResponse, labelsResponse] =
        await Promise.all([imgRequest, labelsRequest]);

    this.datasetLabels = new Uint8Array(await labelsResponse.arrayBuffer());
// Slice the the images and labels into train and test sets.
    this.trainImages =
        this.datasetImages.slice(0, IMAGE_SIZE * NUM_TRAIN_ELEMENTS);
    this.testImages = this.datasetImages.slice(IMAGE_SIZE * NUM_TRAIN_ELEMENTS);
    this.trainLabels =
        this.datasetLabels.slice(0, NUM_CLASSES * NUM_TRAIN_ELEMENTS);
    this.testLabels =
        this.datasetLabels.slice(NUM_CLASSES * NUM_TRAIN_ELEMENTS);
  }

  /**
   * Get all training data as a data tensor and a labels tensor.
   *
   * @returns
   *   xs: The data tensor, of shape `[numTrainExamples, 28, 28, 1]`.
   *   labels: The one-hot encoded labels tensor, of shape
   *     `[numTrainExamples, 10]`.
   */
  getTrainData() {
    const xs = tf.tensor4d(
        this.trainImages,
        [this.trainImages.length / IMAGE_SIZE, IMAGE_H, IMAGE_W, 1]);
    const labels = tf.tensor2d(
        this.trainLabels, [this.trainLabels.length / NUM_CLASSES, NUM_CLASSES]);
    return {xs, labels};
  }

  /**
   * Get all test data as a data tensor a a labels tensor.
   *
   * @param {number} numExamples Optional number of examples to get. If not
   *     provided,
   *   all test examples will be returned.
   * @returns
   *   xs: The data tensor, of shape `[numTestExamples, 28, 28, 1]`.
   *   labels: The one-hot encoded labels tensor, of shape
   *     `[numTestExamples, 10]`.
   */
  getTestData(numExamples) {
    let xs = tf.tensor4d(
        this.testImages,
        [this.testImages.length / IMAGE_SIZE, IMAGE_H, IMAGE_W, 1]);
    let labels = tf.tensor2d(
        this.testLabels, [this.testLabels.length / NUM_CLASSES, NUM_CLASSES]);

    if (numExamples != null) {
      xs = xs.slice([0, 0, 0, 0], [numExamples, IMAGE_H, IMAGE_W, 1]);
      labels = labels.slice([0, 0], [numExamples, NUM_CLASSES]);
    }
    return {xs, labels};
  }
}


 
 
 
 
 
 
var ui = ui || {}    // create ui namespace

const statusElement = document.getElementById('status');
const messageElement = document.getElementById('message');
const imagesElement = document.getElementById('images');


ui.logStatus = function(message) {
  statusElement.innerText = message;
}

  
  
  
  
ui.trainingLog = function(message) {
  messageElement.innerText = `${message}\n`;
  console.log(message);
}

  
  
  
ui.showTestResults = function(batch, predictions, labels) {
  const testExamples = batch.xs.shape[0];
  let totalCorrect = 0;
  for (let i = 0; i < testExamples; i++) {
    const image = batch.xs.slice([i, 0], [1, batch.xs.shape[1]]);

    const div = document.createElement('div');
    div.className = 'pred-container';

    const canvas = document.createElement('canvas');
    canvas.className = 'prediction-canvas';
    ui.draw(image.flatten(), canvas);

    const pred = document.createElement('div');

    const prediction = predictions[i];
    const label = labels[i];
    const correct = prediction === label;

    pred.className = `pred ${(correct ? 'pred-correct' : 'pred-incorrect')}`;
    pred.innerText = `pred: ${prediction}`;

    div.appendChild(pred);
    div.appendChild(canvas);

    imagesElement.appendChild(div);
  }
}

  
  
  
  
const lossLabelElement = document.getElementById('loss-label');
const accuracyLabelElement = document.getElementById('accuracy-label');

const lossValues = {
  train: {
    x: [],
    y: [],
    name: 'train',
    mode: 'lines',
    line: {width: 1}
  },
  validation: {
    x: [],
    y: [],
    name: 'validation',
    mode: 'lines+markers',
    line: {width: 3}
  }
};
  
  
  
  
ui.plotLoss= function(batch, loss, set) {
  lossValues[set].x.push(batch);
  lossValues[set].y.push(loss);
 /* Plotly.newPlot('loss-canvas', [lossValues.train, lossValues.validation], {
    width: 480,
    xaxis: {title: 'batch #'},
    yaxis: {title: 'loss'},
    font: {size: 18}
  });
  */
  lossLabelElement.innerText = `last loss: ${loss}`;
}

  
  
  
  
const accuracyValues = {
  train: {
    x: [],
    y: [],
    name: 'train',
    mode: 'lines',
    line: {width: 1}
  },
  validation: {
    x: [],
    y: [],
    name: 'validation',
    mode: 'lines+markers',
    line: {width: 3}
  }
};
  
  
  
  
ui.plotAccuracy= function(batch, accuracy, set) {
  accuracyValues[set].x.push(batch);
  accuracyValues[set].y.push(accuracy);
/*  Plotly.newPlot(
      'accuracy-canvas',
      [accuracyValues.train, accuracyValues.validation], {
        width: 480,
        xaxis: {title: 'batch #'},
        yaxis: {
          title: 'accuracy',
          range: [0, 1]
        },
        font: {size: 18}
      });
      
      */
  accuracyLabelElement.innerText =
      `last accuracy: ${(accuracy * 100).toFixed(1)}%`;
}

  
  
  
  
  
ui.draw = function(image, canvas) {
  const [width, height] = [28, 28];
  canvas.width = width;
  canvas.height = height;
  const ctx = canvas.getContext('2d');
  const imageData = new ImageData(width, height);
  const data = image.dataSync();
  for (let i = 0; i < height * width; ++i) {
    const j = i * 4;
    imageData.data[j + 0] = data[i] * 255;
    imageData.data[j + 1] = data[i] * 255;
    imageData.data[j + 2] = data[i] * 255;
    imageData.data[j + 3] = 255;
  }
  ctx.putImageData(imageData, 0, 0);
}

  
  
  
ui.getModelTypeId = function() {
  return document.getElementById('model-type').value;
}

ui.getTrainEpochs = function() {
  return Number.parseInt(document.getElementById('train-epochs').value);
}

  
  
  
ui.setTrainButtonCallback= function(callback) {
  const trainButton = document.getElementById('train');
  const modelType = document.getElementById('model-type');
  trainButton.addEventListener('click', () => {
    trainButton.setAttribute('disabled', true);
    modelType.setAttribute('disabled', true);
    callback();
  });
}

  
  
  
  
  
/**
 * Creates a convolutional neural network (Convnet) for the MNIST data.
 *
 * @returns {tf.Model} An instance of tf.Model.
 */
function createConvModel() {
  // Create a sequential neural network model. tf.sequential provides an API
  // for creating "stacked" models where the output from one layer is used as
  // the input to the next layer.
  const model = tf.sequential();

  // The first layer of the convolutional neural network plays a dual role:
  // it is both the input layer of the neural network and a layer that performs
  // the first convolution operation on the input. It receives the 28x28 pixels
  // black and white images. This input layer uses 16 filters with a kernel size
  // of 5 pixels each. It uses a simple RELU activation function which pretty
  // much just looks like this: __/
  model.add(tf.layers.conv2d({
    inputShape: [IMAGE_H, IMAGE_W, 1],
    kernelSize: 3,
    filters: 128,      // was 16
    activation: 'relu'
  }));

  // After the first layer we include a MaxPooling layer. This acts as a sort of
  // downsampling using max values in a region instead of averaging.
  // https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks
  model.add(tf.layers.maxPooling2d({poolSize: 2, strides: 2}));

  // Our third layer is another convolution, this time with 32 filters.
  model.add(tf.layers.conv2d({kernelSize: 3, filters: 32, activation: 'relu'}));
  
  model.add(tf.layers.dropout({ rate : 0.20 }))    // added
  
  // Max pooling again.
  model.add(tf.layers.maxPooling2d({poolSize: 2, strides: 2}));

  // Add another conv2d layer.
  model.add(tf.layers.conv2d({kernelSize: 3, filters: 32, activation: 'relu'}));

 
   
  
  // Now we flatten the output from the 2D filters into a 1D vector to prepare
  // it for input into our last layer. This is common practice when feeding
  // higher dimensional data to a final classification output layer.
  model.add(tf.layers.flatten({}));

  model.add(tf.layers.dense({units: 1000, activation: 'relu'}));
  model.add(tf.layers.dense({units: 100, activation: 'relu'}));

  // Our last layer is a dense layer which has 10 output units, one for each
  // output class (i.e. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9). Here the classes actually
  // represent numbers, but it's the same idea if you had classes that
  // represented other entities like dogs and cats (two output classes: 0, 1).
  // We use the softmax function as the activation for the output layer as it
  // creates a probability distribution over our 10 classes so their output
  // values sum to 1.
  model.add(tf.layers.dense({units: 10, activation: 'softmax'}));

  return model;
}

/**
 * Creates a model consisting of only flatten, dense and dropout layers.
 *
 * The model create here has approximately the same number of parameters
 * (~31k) as the convnet created by `createConvModel()`, but is
 * expected to show a significantly worse accuracy after training, due to the
 * fact that it doesn't utilize the spatial information as the convnet does.
 *
 * This is for comparison with the convolutional network above.
 *
 * @returns {tf.Model} An instance of tf.Model.
 */
function createDenseModel() {
  const model = tf.sequential();
  model.add(tf.layers.flatten({inputShape: [IMAGE_H, IMAGE_W, 1]}));
  model.add(tf.layers.dense({units: 42, activation: 'relu'}));
  model.add(tf.layers.dense({units: 10, activation: 'softmax'}));
  return model;
}

/**
 * Compile and train the given model.
 *
 * @param {*} model The model to
 */
async function train(model) {
  ui.logStatus('Training model...');

  // Now that we've defined our model, we will define our optimizer. The
  // optimizer will be used to optimize our model's weight values during
  // training so that we can decrease our training loss and increase our
  // classification accuracy.

  // The learning rate defines the magnitude by which we update our weights each
  // training step. The higher the value, the faster our loss values converge,
  // but also the more likely we are to overshoot optimal parameters
  // when making an update. A learning rate that is too low will take too long
  // to find optimal (or good enough) weight parameters while a learning rate
  // that is too high may overshoot optimal parameters. Learning rate is one of
  // the most important hyperparameters to set correctly. Finding the right
  // value takes practice and is often best found empirically by trying many
  // values.
  const LEARNING_RATE = 0.01;

  // We are using rmsprop as our optimizer.
  // An optimizer is an iterative method for minimizing an loss function.
  // It tries to find the minimum of our loss function with respect to the
  // model's weight parameters.
  const optimizer = 'rmsprop';

  // We compile our model by specifying an optimizer, a loss function, and a
  // list of metrics that we will use for model evaluation. Here we're using a
  // categorical crossentropy loss, the standard choice for a multi-class
  // classification problem like MNIST digits.
  // The categorical crossentropy loss is differentiable and hence makes
  // model training possible. But it is not amenable to easy interpretation
  // by a human. This is why we include a "metric", namely accuracy, which is
  // simply a measure of how many of the examples are classified correctly.
  // This metric is not differentiable and hence cannot be used as the loss
  // function of the model.
  model.compile({
    optimizer,
    loss: 'categoricalCrossentropy',
    metrics: ['accuracy'],
  });

  // Batch size is another important hyperparameter. It defines the number of
  // examples we group together, or batch, between updates to the model's
  // weights during training. A value that is too low will update weights using
  // too few examples and will not generalize well. Larger batch sizes require
  // more memory resources and aren't guaranteed to perform better.
  const batchSize = 64;

  // Leave out the last 15% of the training data for validation, to monitor
  // overfitting during training.
  const validationSplit = 0.15;

  // Get number of training epochs from the UI.
  const trainEpochs = ui.getTrainEpochs();

  // We'll keep a buffer of loss and accuracy values over time.
  let trainBatchCount = 0;

  const trainData = data.getTrainData();
  const testData = data.getTestData();

  const totalNumBatches =
      Math.ceil(trainData.xs.shape[0] * (1 - validationSplit) / batchSize) *
      trainEpochs;

  // During the long-running fit() call for model training, we include
  // callbacks, so that we can plot the loss and accuracy values in the page
  // as the training progresses.
  let valAcc;
  await model.fit(trainData.xs, trainData.labels, {
    batchSize,
    validationSplit,
    epochs: trainEpochs,
    callbacks: {
      onBatchEnd: async (batch, logs) => {
        trainBatchCount++;
        ui.logStatus(
            `Training... ${trainBatchCount} batches /  ${totalNumBatches} total (` +
            `${(trainBatchCount / totalNumBatches * 100).toFixed(1)}%` +
            ` complete). To stop training, refresh or close page.`);
        ui.plotLoss(trainBatchCount, logs.loss, 'train');
        ui.plotAccuracy(trainBatchCount, logs.acc, 'train');
        await tf.nextFrame();
      },
      onEpochEnd: async (epoch, logs) => {
        valAcc = logs.val_acc;
        ui.plotLoss(trainBatchCount, logs.val_loss, 'validation');
        ui.plotAccuracy(trainBatchCount, logs.val_acc, 'validation');
        await tf.nextFrame();
      }
    }
  });

  const testResult = model.evaluate(testData.xs, testData.labels);
  const testAccPercent = testResult[1].dataSync()[0] * 100;
  const finalValAccPercent = valAcc * 100;
  ui.logStatus(
      `Final validation accuracy: ${finalValAccPercent.toFixed(1)}%; ` +
      `Final test accuracy: ${testAccPercent.toFixed(1)}%`);
}

/**
 * Show predictions on a number of test examples.
 *
 * @param {tf.Model} model The model to be used for making the predictions.
 */
async function showPredictions(model) {
  const testExamples = 100;
  const examples = data.getTestData(testExamples);

  // Code wrapped in a tf.tidy() function callback will have their tensors freed
  // from GPU memory after execution without having to call dispose().
  // The tf.tidy callback runs synchronously.
  tf.tidy(() => {
    const output = model.predict(examples.xs);

    // tf.argMax() returns the indices of the maximum values in the tensor along
    // a specific axis. Categorical classification tasks like this one often
    // represent classes as one-hot vectors. One-hot vectors are 1D vectors with
    // one element for each output class. All values in the vector are 0
    // except for one, which has a value of 1 (e.g. [0, 0, 0, 1, 0]). The
    // output from model.predict() will be a probability distribution, so we use
    // argMax to get the index of the vector element that has the highest
    // probability. This is our prediction.
    // (e.g. argmax([0.07, 0.1, 0.03, 0.75, 0.05]) == 3)
    // dataSync() synchronously downloads the tf.tensor values from the GPU so
    // that we can use them in our normal CPU JavaScript code
    // (for a non-blocking version of this function, use data()).
    const axis = 1;
    const labels = Array.from(examples.labels.argMax(axis).dataSync());
    const predictions = Array.from(output.argMax(axis).dataSync());

    ui.showTestResults(examples, predictions, labels);
    
  });
  
   
}

function createModel() {
  let model;
  const modelType = ui.getModelTypeId();
  if (modelType === 'ConvNet') {
    model = createConvModel();
  } else if (modelType === 'DenseNet') {
    model = createDenseModel();
  } else {
    throw new Error(`Invalid model type: ${modelType}`);
  }
  return model;
}

let data;
async function load() {
  data = new MnistData();
  await data.load();
}

// This is our main function. It loads the MNIST data, trains the model, and
// then shows what the model predicted on unseen test data.
ui.setTrainButtonCallback(async () => {
  ui.logStatus('Loading MNIST data...');
  await load();

  ui.logStatus('Creating model...');
  const model = createModel();
 // model.summary();
  document.getElementById('myDivSummary').innerHTML = ''      
  await model.summary(null,null,x => {document.getElementById('myDivSummary').innerHTML += x + '<br>'});

  ui.logStatus('Starting model training...');
  const startTime = new Date();
  await train(model);
  const endTime = new Date();
   document.getElementById('myDurationOut').innerHTML += '<hr>Train duration was:'+ ((endTime-startTime)/1000).toFixed(0) + ' seconds<br>'


  showPredictions(model);
   const saveResults = await model.save('downloads://my-mnist01');
});
  
  
  </script>

    
    
    
    
    
    
    
    
    
    
    
    <!--
    <script src="https://cdn.jsdelivr.net/npm/vega@3.2.1/build/vega.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/vega-lite@2.3.1/build/vega-lite.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/vega-embed@3.2.0/build/vega-embed.js"></script>



    <script src="data.js"></script>
    <script src="ui.js"></script>

    <script src="index.js"></script>
   -->



  </body>
</html>
